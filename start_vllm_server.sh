#!/bin/bash
# vLLMæœåŠ¡å™¨å¯åŠ¨è„šæœ¬ - ä¸ºç¾½æ¯›çƒæ£€æµ‹å™¨æä¾›APIæœåŠ¡

# é»˜è®¤é…ç½®
MODEL=${1:-"Qwen/Qwen3-VL-8B-Instruct"}
HOST=${2:-"0.0.0.0"}
PORT=${3:-8000}
GPU_MEMORY=${4:-0.8}

echo "ğŸš€ å¯åŠ¨vLLMæœåŠ¡å™¨..."
echo "æ¨¡å‹: $MODEL"
echo "åœ°å€: $HOST:$PORT"
echo "GPUå†…å­˜ä½¿ç”¨ç‡: $GPU_MEMORY"

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"

# ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒä¸­çš„vllmç›´æ¥æ‰§è¡Œ
"$SCRIPT_DIR/.venv/bin/vllm" serve $MODEL \
    --host $HOST \
    --port $PORT \
    --max-model-len 128000 \
    --gpu-memory-utilization 0.96 \
    --media-io-kwargs '{"video": {"num_frames": -1}}' \
    --enable-log-requests \
    --uvicorn-log-level debug

echo "âœ… vLLMæœåŠ¡å™¨å·²å¯åŠ¨åœ¨ http://$HOST:$PORT"
echo "ğŸ’¡ æµ‹è¯•å‘½ä»¤:"
echo "   python src/universal_rally_detector.py --video test.mp4 --vllm-api-base http://localhost:8000/v1"
